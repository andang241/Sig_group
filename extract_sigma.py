import requests
import json
import pandas as pd
from getpass import getpass
import warnings
from typing import Dict, List, Any, Generator, Set, Tuple
from difflib import SequenceMatcher
import random
import re
from datetime import datetime, timedelta
from collections import defaultdict
import gc
import hashlib

# --- CONFIGURATION ---

# 1. Th√¥ng tin k·∫øt n·ªëi Wazuh Indexer
INDEXER_DOMAIN = "https://wazuh-indexer.misaonline.vpnlocal"
INDEXER_USER = 'dban'

# 2. Danh s√°ch c√°c Rule ID b·∫°n mu·ªën theo d√µi
RULES_TO_MONITOR = [
    "142749"
]

# 3. Streaming configuration - C·∫•u h√¨nh x·ª≠ l√Ω d·ªØ li·ªáu theo chunks
CHUNK_SIZE = 5000                    # S·ªë events x·ª≠ l√Ω m·ªói l·∫ßn
MAX_GROUPS_IN_MEMORY = 15000         # Gi·ªõi h·∫°n s·ªë groups trong memory
MEMORY_CLEANUP_INTERVAL = 50000      # T·∫ßn su·∫•t cleanup memory

# 4. Enhanced deduplication configuration - C·∫•u h√¨nh dedup cho t·ª´ng lo·∫°i event
DEDUP_CONFIG_BY_GROUP_ENHANCED = {
    "sigmaprocess_creation": {
        "event_type": "sigmaprocess_creation",
        "primary_fields": [                    # C√°c tr∆∞·ªùng CH√çNH ƒë·ªÉ nh√≥m events
            "data.win.eventdata.parentImage", # Process cha (v√≠ d·ª•: forfiles.exe)
            "data.win.eventdata.user"         # User th·ª±c thi (v√≠ d·ª•: NT AUTHORITY\SYSTEM)
        ],
        "similarity_fields": [                # C√°c tr∆∞·ªùng ƒë·ªÉ so s√°nh similarity
            "data.win.eventdata.commandLine",      # Command line
            "data.win.eventdata.parentCommandLine" # Parent command line
        ],
        "max_samples_per_group": 10,         # S·ªë events t·ªëi ƒëa trong 1 group
        "similarity_threshold": 0.75,        # Ng∆∞·ª°ng similarity ƒë·ªÉ nh√≥m (75%)
        "use_enhanced_similarity": True,     # S·ª≠ d·ª•ng enhanced similarity
        "time_window_minutes": 120,          # Time window ƒë·ªÉ nh√≥m (2 gi·ªù)
        "use_semantic_similarity": True      # S·ª≠ d·ª•ng semantic similarity
    },
    "network_connection": {
        "event_type": "network_connection",
        "primary_fields": [
            "data.win.eventdata.image",
            "data.win.eventdata.protocol"
        ],
        "similarity_fields": [
            "data.win.eventdata.destinationIp",
            "data.win.eventdata.destinationPort",
            "data.win.eventdata.destinationHostname"
        ],
        "max_samples_per_group": 5,
        "similarity_threshold": 0.80,  # Reduced from 0.90
        "use_enhanced_similarity": False,
        "time_window_minutes": 60,  # Increased from 30
        "use_semantic_similarity": False
    },
    "file_event": {
        "event_type": "file_event",
        "primary_fields": [
            "data.win.eventdata.image"
        ],
        "similarity_fields": [
            "data.win.eventdata.targetFilename",
            "data.win.eventdata.image"              
        ],
        "max_samples_per_group": 8,  # Increased from 5
        "similarity_threshold": 0.85,  # Reduced from 0.85
        "use_enhanced_similarity": True,
        "time_window_minutes": 90,  # Increased from 45
        "use_semantic_similarity": True
    },
    "registry_set": {
        "event_type": "registry_set",
        "primary_fields": [
            "data.win.eventdata.image",
            "data.win.eventdata.eventType"
        ],
        "similarity_fields": [
            "data.win.eventdata.targetObject"
        ],
        "max_samples_per_group": 5,
        "similarity_threshold": 0.75,  # Reduced from 0.85
        "use_enhanced_similarity": False,
        "time_window_minutes": 60,  # Increased from 30
        "use_semantic_similarity": False
    },
    "default": {
        "event_type": "generic",
        "primary_fields": ["rule.id", "rule.level", "agent.name"],
        "similarity_fields": ["full_log"],
        "max_samples_per_group": 8,  # Increased from 5
        "similarity_threshold": 0.75,  # Reduced from 0.95 for better grouping
        "use_enhanced_similarity": True,
        "time_window_minutes": 120,  # Increased from 60
        "use_semantic_similarity": True
    }
}

# 5. C√°c c·∫•u h√¨nh kh√°c
FETCH_SIZE_PER_PAGE = CHUNK_SIZE
TIME_RANGE = "now-1d"  # UTC+7 (Gi·ªù Vi·ªát Nam) - L·∫•y d·ªØ li·ªáu 1 ng√†y tr∆∞·ªõc theo gi·ªù Vi·ªát Nam
# Gi·∫£i th√≠ch: 
# - now: Th·ªùi ƒëi·ªÉm hi·ªán t·∫°i
# - -1d: Tr·ª´ ƒëi 1 ng√†y  
# - /d: L√†m tr√≤n xu·ªëng ƒë·∫øn ƒë·∫ßu ng√†y (00:00:00)
# - +07:00: Chuy·ªÉn v·ªÅ UTC+7 (Gi·ªù Vi·ªát Nam)
# V√≠ d·ª•: N·∫øu b√¢y gi·ªù l√† 18/08/2025 22:30 (gi·ªù Vi·ªát Nam)
# ‚Üí Script s·∫Ω query t·ª´ 17/08/2025 00:00 (gi·ªù Vi·ªát Nam) ƒë·∫øn hi·ªán t·∫°i
USER_AGENT = 'curl/7.81.0'
MAX_EVENTS_TO_PROCESS = 2000000

# T·∫Øt c·∫£nh b√°o SSL
warnings.filterwarnings("ignore", category=requests.packages.urllib3.exceptions.InsecureRequestWarning)


# --- UTILITY FUNCTIONS ---

def get_nested_value(obj: Dict, path: str) -> Any:
    """
    L·∫•y gi√° tr·ªã nested t·ª´ dictionary theo ƒë∆∞·ªùng d·∫´n d·∫°ng 'data.win.eventdata.parentImage'
    
    Args:
        obj: Dictionary ch·ª©a d·ªØ li·ªáu
        path: ƒê∆∞·ªùng d·∫´n d·∫°ng 'field1.field2.field3'
    
    Returns:
        Gi√° tr·ªã t·∫°i ƒë∆∞·ªùng d·∫´n ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
    
    V√≠ d·ª•:
        get_nested_value(event, 'data.win.eventdata.parentImage')
        ‚Üí Tr·∫£ v·ªÅ 'C:\\Windows\\System32\\forfiles.exe'
    """
    keys = path.split('.')
    current = obj
    for key in keys:
        if isinstance(current, dict) and key in current:
            current = current[key]
        else: 
            return None
    return current

def safe_str(value: Any) -> str:
    """
    Chuy·ªÉn ƒë·ªïi an to√†n sang string, tr√°nh l·ªói khi value l√† None
    
    Args:
        value: Gi√° tr·ªã c·∫ßn chuy·ªÉn ƒë·ªïi
    
    Returns:
        String representation c·ªßa value, ho·∫∑c "" n·∫øu value l√† None
    """
    if value is None:
        return ""
    return str(value)


# --- ENHANCED SIMILARITY FUNCTIONS ---

def normalize_command_line_enhanced(cmd_line: str) -> str:
    """
    Chu·∫©n h√≥a command line ƒë·ªÉ so s√°nh similarity ch√≠nh x√°c h∆°n
    
    M·ª•c ƒë√≠ch: Lo·∫°i b·ªè c√°c th√¥ng tin kh√¥ng c·∫ßn thi·∫øt (GUIDs, timestamps, ƒë∆∞·ªùng d·∫´n c·ª• th·ªÉ)
    ƒë·ªÉ gi·ªØ l·∫°i c·∫•u tr√∫c v√† logic c·ªßa command.
    
    Args:
        cmd_line: Command line g·ªëc
    
    Returns:
        Command line ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a
    
    V√≠ d·ª•:
        Input:  'forfiles /p "C:\\temp" /s /d -3 /c "cmd /c del \"Bang_ke_1755415510625.xls\" /q"'
        Output: 'forfiles /p QUOTED_PATH /s /d -3 /c "cmd /c del \\QUOTED_PATH /q"'
    """
    if not cmd_line:
        return ""
    
    try:
        cmd = cmd_line.lower().strip()
        
        # 1. Normalize conditional statements - Chu·∫©n h√≥a c√°c c√¢u l·ªánh ƒëi·ªÅu ki·ªán
        cmd = re.sub(r'if\s+(true|false)==(true|false)', 'if CONDITION', cmd)
        cmd = re.sub(r'if\s+/i\s+not', 'if not', cmd)
        
        # 2. Replace ALL GUIDs with consistent pattern - Thay th·∫ø GUIDs b·∫±ng pattern nh·∫•t qu√°n
        cmd = re.sub(r'\b[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}\b', 'GUID', cmd)
        
        # 3. Normalize file extensions and paths - Chu·∫©n h√≥a extensions v√† ƒë∆∞·ªùng d·∫´n
        cmd = re.sub(r'_\d{8}_\d{6,}', '_TIMESTAMP', cmd)  # file timestamps
        cmd = re.sub(r'_\d{8}\.', '_DATE.', cmd)  # date patterns
        cmd = re.sub(r'\d{2}\.\d{2}\.\d{4}', 'DATE', cmd)  # DD.MM.YYYY
        cmd = re.sub(r'\d{4}-\d{2}-\d{2}', 'DATE', cmd)    # YYYY-MM-DD
        
        # 4. Replace specific Vietnamese file patterns - Thay th·∫ø pattern file ti·∫øng Vi·ªát
        cmd = re.sub(r'"[^"]*\.(xlsx?|docx?|pdf|txt|log)"', 'DOCUMENT', cmd)
        cmd = re.sub(r'"[^"]*_\d+\.(xlsx?|docx?|pdf)"', 'DOCUMENT', cmd)
        
        # 5. Normalize numbers and IDs - Chu·∫©n h√≥a s·ªë v√† IDs
        cmd = re.sub(r'\b\d{8,}\b', 'LONGID', cmd)  # Long numeric IDs
        cmd = re.sub(r'\b\d{5,7}\b', 'MEDIUMID', cmd)  # Medium IDs
        
        # 6. Normalize Vietnamese document names patterns - Chu·∫©n h√≥a t√™n t√†i li·ªáu ti·∫øng Vi·ªát
        cmd = re.sub(r'"[^"]*thang\s+\d+[^"]*"', 'MONTHLY_REPORT', cmd)
        cmd = re.sub(r'"[^"]*chi_tiet[^"]*"', 'DETAIL_REPORT', cmd)
        cmd = re.sub(r'"[^"]*bang_ke[^"]*"', 'SUMMARY_REPORT', cmd)
        
        # 7. Replace quoted file paths entirely - Thay th·∫ø to√†n b·ªô ƒë∆∞·ªùng d·∫´n trong d·∫•u ngo·∫∑c k√©p
        cmd = re.sub(r'"[^"]*"', 'QUOTED_PATH', cmd)
        
        # 8. Normalize drives and common paths - Chu·∫©n h√≥a ·ªï ƒëƒ©a v√† ƒë∆∞·ªùng d·∫´n chung
        cmd = re.sub(r'[c-z]:\\[^\s]*', 'FILEPATH', cmd)
        
        # 9. Clean up multiple spaces - D·ªçn d·∫πp kho·∫£ng tr·∫Øng th·ª´a
        cmd = ' '.join(cmd.split())
        
        return cmd
        
    except Exception as e:
        print(f"Warning: L·ªói normalize_command_line_enhanced: {e}")
        return cmd_line.lower().strip()

def normalize_command_line(cmd_line: str) -> str:
    """Wrapper function - s·ª≠ d·ª•ng enhanced version"""
    return normalize_command_line_enhanced(cmd_line)

def extract_command_tokens(cmd_line: str) -> Set[str]:
    """
    Tr√≠ch xu·∫•t c√°c token quan tr·ªçng t·ª´ command line ƒë·ªÉ t√≠nh Jaccard similarity
    
    Args:
        cmd_line: Command line c·∫ßn tr√≠ch xu·∫•t tokens
    
    Returns:
        Set c√°c tokens c√≥ √Ω nghƒ©a (lo·∫°i b·ªè stop words)
    
    V√≠ d·ª•:
        Input:  'cmd /c del /q "C:\\temp\\file.txt"'
        Output: {'cmd', 'del', 'temp', 'file', 'txt'}
    """
    if not cmd_line:
        return set()
    
    # T√°ch th√†nh tokens
    tokens = re.findall(r'[\w\-\.]+', cmd_line.lower())
    
    # Lo·∫°i b·ªè tokens kh√¥ng quan tr·ªçng
    ignore_tokens = {'exe', 'dll', 'com', 'bat', 'ps1', 'vbs', 'js', 'the', 'and', 'or', 'to', 'in', 'of', 'is', 'it'}
    meaningful_tokens = {token for token in tokens if len(token) > 2 and token not in ignore_tokens}
    
    return meaningful_tokens

def extract_command_structure(cmd_line: str) -> str:
    """
    Tr√≠ch xu·∫•t c·∫•u tr√∫c l·ªánh c·ªët l√µi (lo·∫°i b·ªè c√°c gi√° tr·ªã c·ª• th·ªÉ)
    
    Args:
        cmd_line: Command line g·ªëc
    
    Returns:
        C·∫•u tr√∫c command ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a
    
    V√≠ d·ª•:
        Input:  'forfiles /p "C:\\temp" /s /d -3 /c "cmd /c del @file /q"'
        Output: 'forfiles /p FILEPATH /s /d -3 /c "cmd /c del @file /q"'
    """
    if not cmd_line:
        return ""
    
    structure = cmd_line.lower()
    
    # Thay th·∫ø to√†n b·ªô ƒë∆∞·ªùng d·∫´n b·∫±ng PATH placeholder
    structure = re.sub(r'"[c-z]:\\[^"]*"', 'FILEPATH', structure)
    structure = re.sub(r'[c-z]:\\[^\s]*', 'FILEPATH', structure)
    
    # Thay th·∫ø parameters v·ªõi values
    structure = re.sub(r'-\w+\s+[^\s-]+', 'PARAM VALUE', structure)
    
    return structure.strip()

def extract_command_pattern_enhanced(cmd_line: str) -> str:
    """
    Ph√°t hi·ªán pattern c·ªßa command ƒë·ªÉ nh√≥m c√°c commands t∆∞∆°ng t·ª±
    
    M·ª•c ƒë√≠ch: Ph√¢n lo·∫°i commands theo m·ª•c ƒë√≠ch s·ª≠ d·ª•ng (file deletion, cleanup, etc.)
    
    Args:
        cmd_line: Command line c·∫ßn ph√¢n lo·∫°i
    
    Returns:
        Pattern string m√¥ t·∫£ lo·∫°i command
    
    V√≠ d·ª•:
        'forfiles /p C:\temp /s /d -3 /c "cmd /c del @file /q"' ‚Üí 'forfiles_scheduled_cleanup'
        'cmd /c del /q file.txt' ‚Üí 'quiet_file_deletion'
    """
    if not cmd_line:
        return "empty"
    
    cmd = cmd_line.lower().strip()
    
    # Special handling for forfiles.exe patterns - X·ª≠ l√Ω ƒë·∫∑c bi·ªát cho forfiles.exe
    if "forfiles" in cmd:
        if "del" in cmd and "/d -" in cmd:
            return "forfiles_scheduled_cleanup"  # forfiles with date-based deletion
        elif "del" in cmd:
            return "forfiles_file_deletion"      # forfiles with deletion
        else:
            return "forfiles_generic"            # other forfiles usage
    
    # Specific business patterns first - C√°c pattern nghi·ªáp v·ª• c·ª• th·ªÉ
    if "if true==false" in cmd or "if false==false" in cmd:
        if "del" in cmd:
            return "conditional_delete_safe"  # Safe delete (never executes)
        elif "if /i not" in cmd:
            return "conditional_file_check_safe"  # Safe file existence check
    
    # General patterns - C√°c pattern chung
    if "cmd /c" in cmd and "del" in cmd and "/q" in cmd:
        return "silent_file_delete"
    elif "powershell" in cmd:
        if "invoke-webrequest" in cmd or "downloadstring" in cmd:
            return "powershell_download"
        elif "-enc" in cmd:
            return "powershell_encoded"
        else:
            return "powershell_generic"
    elif "rundll32" in cmd:
        return "rundll32_execution"
    elif "tasklist" in cmd:
        return "process_enumeration"
    elif "net user" in cmd:
        return "user_management"
    elif "reg add" in cmd or "reg delete" in cmd:
        return "registry_modification"
    elif "del" in cmd and "/q" in cmd:
        return "quiet_file_deletion"
    elif "del" in cmd:
        return "file_deletion"
    else:
        return "generic"

def extract_command_pattern(cmd_line: str) -> str:
    """Wrapper function - s·ª≠ d·ª•ng enhanced version"""
    return extract_command_pattern_enhanced(cmd_line)

def calculate_jaccard_similarity(set1: Set, set2: Set) -> float:
    """
    T√≠nh Jaccard similarity coefficient gi·ªØa 2 sets
    
    C√¥ng th·ª©c: J(A,B) = |A ‚à© B| / |A ‚à™ B|
    
    Args:
        set1: Set th·ª© nh·∫•t
        set2: Set th·ª© hai
    
    Returns:
        Gi√° tr·ªã similarity t·ª´ 0.0 ƒë·∫øn 1.0
    
    V√≠ d·ª•:
        set1 = {'cmd', 'del', 'file'}
        set2 = {'cmd', 'del', 'temp'}
        ‚Üí Jaccard = 2/4 = 0.5
    """
    if not set1 and not set2:
        return 1.0
    if not set1 or not set2:
        return 0.0
    
    intersection = len(set1.intersection(set2))
    union = len(set1.union(set2))
    
    return intersection / union if union > 0 else 0.0

def calculate_semantic_similarity(str1: str, str2: str) -> float:
    """
    T√≠nh semantic similarity n√¢ng cao cho Vietnamese file names
    
    M·ª•c ƒë√≠ch: So s√°nh √Ω nghƒ©a th·ª±c s·ª± c·ªßa commands, kh√¥ng ch·ªâ text matching
    
    Args:
        str1: String th·ª© nh·∫•t
        str2: String th·ª© hai
    
    Returns:
        Gi√° tr·ªã similarity t·ª´ 0.0 ƒë·∫øn 1.0
    """
    if not str1 or not str2:
        return 0.0
    
    # Normalize both strings
    norm1 = normalize_command_line_enhanced(str1)
    norm2 = normalize_command_line_enhanced(str2)
    
    # Exact match after normalization
    if norm1 == norm2:
        return 1.0
    
    # Pattern-based similarity
    pattern1 = extract_command_pattern_enhanced(str1)
    pattern2 = extract_command_pattern_enhanced(str2)
    
    if pattern1 == pattern2:
        # Same pattern, check structural similarity
        return SequenceMatcher(None, norm1, norm2).ratio()
    else:
        # Different patterns
        return 0.0

def calculate_enhanced_similarity(str1: str, str2: str) -> float:
    """
    T√≠nh similarity b·∫±ng c√°ch k·∫øt h·ª£p nhi·ªÅu ph∆∞∆°ng ph√°p
    
    Strategy: K·∫øt h·ª£p pattern matching, structural similarity, normalized similarity, v√† token-based similarity
    
    Args:
        str1: String th·ª© nh·∫•t
        str2: String th·ª© hai
    
    Returns:
        Gi√° tr·ªã similarity t·ª´ 0.0 ƒë·∫øn 1.0
    """
    if not str1 or not str2:
        return 0.0
    
    try:
        # 1. Command pattern matching (highest priority - 35%)
        pattern1 = extract_command_pattern_enhanced(str1)
        pattern2 = extract_command_pattern_enhanced(str2)
        pattern_sim = 1.0 if pattern1 == pattern2 else 0.0
        
        # N·∫øu patterns kh√°c nhau ho√†n to√†n, similarity th·∫•p
        if pattern_sim == 0.0 and pattern1 != "generic" and pattern2 != "generic":
            return 0.0
        
        # 2. Structural similarity (25%)
        struct1 = extract_command_structure(str1)
        struct2 = extract_command_structure(str2)
        struct_sim = SequenceMatcher(None, struct1, struct2).ratio()
        
        # 3. Normalized similarity (25%)
        norm_str1 = normalize_command_line_enhanced(str1)
        norm_str2 = normalize_command_line_enhanced(str2)
        norm_sim = SequenceMatcher(None, norm_str1, norm_str2).ratio()
        
        # 4. Token-based similarity (15%)
        tokens1 = extract_command_tokens(norm_str1)
        tokens2 = extract_command_tokens(norm_str2)
        jaccard_sim = calculate_jaccard_similarity(tokens1, tokens2)
        
        # Weighted combination - K·∫øt h·ª£p c√≥ tr·ªçng s·ªë
        final_similarity = (pattern_sim * 0.35 + struct_sim * 0.25 + norm_sim * 0.25 + jaccard_sim * 0.15)
        
        return final_similarity
        
    except Exception as e:
        print(f"Warning: L·ªói calculate_enhanced_similarity: {e}")
        return 0.0

def calculate_standard_similarity(str1: str, str2: str) -> float:
    """
    Similarity calculation chu·∫©n (faster) - S·ª≠ d·ª•ng SequenceMatcher ƒë∆°n gi·∫£n
    
    Args:
        str1: String th·ª© nh·∫•t
        str2: String th·ª© hai
    
    Returns:
        Gi√° tr·ªã similarity t·ª´ 0.0 ƒë·∫øn 1.0
    """
    if not str1 or not str2:
        return 0.0
    
    # Normalize v√† d√πng SequenceMatcher
    norm_str1 = normalize_command_line_enhanced(str1)
    norm_str2 = normalize_command_line_enhanced(str2)
    
    return SequenceMatcher(None, norm_str1, norm_str2).ratio()


# --- RULE GROUP FUNCTIONS ---

def get_rule_groups_from_event(event: Dict) -> List[str]:
    """
    L·∫•y rule groups t·ª´ event ƒë·ªÉ x√°c ƒë·ªãnh lo·∫°i event
    
    Args:
        event: Event dictionary t·ª´ Wazuh
    
    Returns:
        List c√°c rule groups (v√≠ d·ª•: ['process_creation', 'windows'])
    
    V√≠ d·ª•:
        event['rule']['groups'] = ['process_creation', 'windows', 'file_operation']
        ‚Üí Returns: ['process_creation', 'windows', 'file_operation']
    """
    return event.get('rule', {}).get('groups', [])

def determine_dedup_config(event: Dict) -> Dict:
    """
    X√°c ƒë·ªãnh config dedup d·ª±a tr√™n rule groups c·ªßa event
    
    M·ª•c ƒë√≠ch: M·ªói lo·∫°i event (process_creation, network_connection, etc.) 
    c√≥ config dedup ri√™ng ph√π h·ª£p v·ªõi ƒë·∫∑c ƒëi·ªÉm c·ªßa n√≥.
    
    Args:
        event: Event dictionary t·ª´ Wazuh
    
    Returns:
        Config dedup ph√π h·ª£p cho lo·∫°i event n√†y
    
    V√≠ d·ª•:
        - process_creation ‚Üí DEDUP_CONFIG_BY_GROUP_ENHANCED["process_creation"]
        - network_connection ‚Üí DEDUP_CONFIG_BY_GROUP_ENHANCED["network_connection"]
        - Kh√¥ng match ‚Üí DEDUP_CONFIG_BY_GROUP_ENHANCED["default"]
    """
    # Debug: In ra th√¥ng tin event ƒë·ªÉ ki·ªÉm tra
    # print(f"\nüîç DEBUG - Event info:")
    # print(f"  ‚Ä¢ Rule ID: {get_nested_value(event, 'rule.id')}")
    # print(f"  ‚Ä¢ Rule Description: {get_nested_value(event, 'rule.description')}")
    # print(f"  ‚Ä¢ Image: {get_nested_value(event, 'data.win.eventdata.image')}")
    # print(f"  ‚Ä¢ User: {get_nested_value(event, 'data.win.eventdata.user')}")
    # print(f"  ‚Ä¢ ParentImage: {get_nested_value(event, 'data.win.eventdata.parentImage')}")
    
    rule_groups = get_rule_groups_from_event(event)
    print(f"  ‚Ä¢ Rule Groups: {rule_groups}")
    
    # Priority order cho vi·ªác matching groups
    group_priority = ['sigmaprocess_creation', 'network_connection', 'registry_set', 'file_event']
    
    for priority_group in group_priority:
        if priority_group in rule_groups:
            config = DEDUP_CONFIG_BY_GROUP_ENHANCED.get(priority_group, DEDUP_CONFIG_BY_GROUP_ENHANCED["default"])
            print(f"  ‚Ä¢ ‚úÖ Using config: {priority_group}")
            print(f"  ‚Ä¢ Primary fields: {config.get('primary_fields', [])}")
            return config
    
    # Fallback to default
    config = DEDUP_CONFIG_BY_GROUP_ENHANCED["default"]
    print(f"  ‚Ä¢ ‚ö†Ô∏è Using default config")
    print(f"  ‚Ä¢ Primary fields: {config.get('primary_fields', [])}")
    return config


# --- TIME-BASED GROUPING ---

def group_events_by_time(events: List[Dict], time_window_minutes: int = 30) -> Dict[str, List[Dict]]:
    """
    Nh√≥m events theo time windows ƒë·ªÉ x·ª≠ l√Ω hi·ªáu qu·∫£ h∆°n
    
    M·ª•c ƒë√≠ch: 
    1. Gi·∫£m memory usage b·∫±ng c√°ch x·ª≠ l√Ω theo chunks th·ªùi gian
    2. Nh√≥m events g·∫ßn nhau v·ªÅ th·ªùi gian (c√≥ th·ªÉ li√™n quan ƒë·∫øn nhau)
    3. Tr√°nh so s√°nh events c√°ch xa nhau v·ªÅ th·ªùi gian
    
    Args:
        events: List c√°c events c·∫ßn nh√≥m
        time_window_minutes: Kho·∫£ng th·ªùi gian m·ªói window (ph√∫t)
    
    Returns:
        Dictionary v·ªõi key l√† time window, value l√† list events trong window ƒë√≥
    
    V√≠ d·ª•:
        time_window_minutes = 60
        ‚Üí Events t·ª´ 18:00-19:00 ‚Üí "2025-08-17_18:00"
        ‚Üí Events t·ª´ 19:00-20:00 ‚Üí "2025-08-17_19:00"
    """
    time_groups = defaultdict(list)
    
    for event in events:
        timestamp_str = event.get('@timestamp')
        if not timestamp_str:
            time_groups['no_timestamp'].append(event)
            continue
            
        try:
            # Parse timestamp
            dt = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
            
            # L√†m tr√≤n v·ªÅ time window
            minutes_rounded = (dt.minute // time_window_minutes) * time_window_minutes
            rounded_time = dt.replace(minute=minutes_rounded, second=0, microsecond=0)
            
            time_key = rounded_time.strftime('%Y-%m-%d_%H:%M')
            time_groups[time_key].append(event)
            
        except Exception as e:
            time_groups['invalid_timestamp'].append(event)
    
    return dict(time_groups)


# --- ENHANCED GROUP KEY GENERATION ---

def create_enhanced_group_key(event: Dict, config: Dict) -> str:
    """
    T·∫°o group key v·ªõi PRIORITY cho primary fields, commandLine ch·ªâ ƒë·ªÉ refine
    
    Strategy: 
    1. PRIMARY FIELDS FIRST: parentImage + user (v√≠ d·ª•: forfiles.exe + NT AUTHORITY\SYSTEM)
    2. COMMAND PATTERN SECONDARY: Ch·ªâ ƒë·ªÉ organize trong primary group
    
    Args:
        event: Event dictionary t·ª´ Wazuh
        config: Config dedup cho lo·∫°i event n√†y
    
    Returns:
        Group key string d·∫°ng "hash_pattern"
    
    V√≠ d·ª•:
        - parentImage: "C:\\Windows\\System32\\forfiles.exe"
        - user: "NT AUTHORITY\\SYSTEM"
        - pattern: "forfiles_scheduled_cleanup"
        ‚Üí Returns: "a1b2c3d4_forfiles_scheduled_cleanup"
    """
    primary_fields = config.get("primary_fields", [])
    
    # PRIORITY 1: Primary fields (parentImage, user, etc.) - MUST MATCH
    base_parts = []
    for field in primary_fields:
        value = safe_str(get_nested_value(event, field))
        if not value:  # If primary field is missing, use placeholder
            value = "MISSING"
        base_parts.append(value)
    
    # Debug: In ra primary fields values
    # print(f"  üîë DEBUG - Primary fields values:")
    # for i, field in enumerate(primary_fields):
    #     print(f"    ‚Ä¢ {field}: '{base_parts[i]}'")
    
    # PRIORITY 2: Command pattern (only for refinement, not primary grouping)
    cmd_line = safe_str(get_nested_value(event, 'data.win.eventdata.commandLine'))
    pattern = extract_command_pattern_enhanced(cmd_line)
    
    # Create hash for consistent grouping - PRIMARY FIELDS FIRST
    group_str = "|".join(base_parts)  # Only primary fields for main grouping
    group_hash = hashlib.md5(group_str.encode()).hexdigest()[:8]
    
    final_key = f"{group_hash}_{pattern}"
    print(f"    ‚Ä¢ Group string: {group_str}")
    print(f"    ‚Ä¢ Group hash: {group_hash}")
    print(f"    ‚Ä¢ Pattern: {pattern}")
    print(f"    ‚Ä¢ Final key: {final_key}")
    
    # Pattern is secondary - only used for internal organization
    return final_key


# --- ENHANCED STREAMING DEDUPLICATION CLASS ---

class StreamingDeduplicatorV2:
    """
    Class ch√≠nh x·ª≠ l√Ω deduplication streaming v·ªõi memory management
    
    M·ª•c ƒë√≠ch: 
    1. X·ª≠ l√Ω events theo chunks ƒë·ªÉ ti·∫øt ki·ªám memory
    2. Merge c√°c groups t·ª´ chunks kh√°c nhau
    3. Cleanup memory ƒë·ªãnh k·ª≥ ƒë·ªÉ tr√°nh memory overflow
    4. T·∫°o final results v·ªõi metadata ƒë·∫ßy ƒë·ªß
    
    Attributes:
        config: Config dedup cho lo·∫°i event
        global_groups: Dictionary ch·ª©a t·∫•t c·∫£ groups ƒë√£ x·ª≠ l√Ω
        processed_count: S·ªë events ƒë√£ x·ª≠ l√Ω
    """
    
    def __init__(self, config: Dict):
        """
        Kh·ªüi t·∫°o deduplicator v·ªõi config
        
        Args:
            config: Config dedup t·ª´ DEDUP_CONFIG_BY_GROUP_ENHANCED
        """
        self.config = config
        self.global_groups = {}  # L∆∞u tr·ªØ t·∫•t c·∫£ groups
        self.processed_count = 0 # ƒê·∫øm s·ªë events ƒë√£ x·ª≠ l√Ω
        
    def merge_groups_into_global(self, chunk_groups: List[Dict]) -> None:
        """
        Merge c√°c groups t·ª´ chunk hi·ªán t·∫°i v√†o global groups
        
        M·ª•c ƒë√≠ch: T√≠ch l≈©y th√¥ng tin t·ª´ c√°c chunks kh√°c nhau ƒë·ªÉ c√≥ c√°i nh√¨n t·ªïng quan
        
        Args:
            chunk_groups: List c√°c events ƒë√£ ƒë∆∞·ª£c dedup trong chunk hi·ªán t·∫°i
        """
        # print(f"\nüîÑ DEBUG - Merging {len(chunk_groups)} groups into global")
        
        for event in chunk_groups:
            group_info = event.get('_group_info', {})
            pattern = group_info.get('pattern', 'unknown')
            primary_key = group_info.get('primary_key', 'unknown')  # L·∫•y primary key
            
            # Create semantic key for global grouping
            cmd_line = safe_str(get_nested_value(event, 'data.win.eventdata.commandLine'))
            normalized_cmd = normalize_command_line_enhanced(cmd_line)
            
            # T·∫°o key duy nh·∫•t cho global group - S·ª¨ D·ª§NG PRIMARY KEY + PATTERN
            global_key = f"{primary_key}:{pattern}"
            
            print(f"  ‚Ä¢ Processing: {primary_key} | {pattern}")
            print(f"    - Global key: {global_key}")
            print(f"    - Events in group: {group_info.get('total_events_in_group', 1)}")
            
            if global_key in self.global_groups:
                # Merge v√†o existing group
                old_total = self.global_groups[global_key]['total_events']
                self.global_groups[global_key]['total_events'] += group_info.get('total_events_in_group', 1)
                print(f"    - ‚úÖ Merged into existing group: {old_total} ‚Üí {self.global_groups[global_key]['total_events']} events")
            else:
                # T·∫°o global group m·ªõi
                self.global_groups[global_key] = {
                    'representative': event,
                    'total_events': group_info.get('total_events_in_group', 1),
                    'first_seen': group_info.get('first_seen', ''),
                    'last_seen': group_info.get('last_seen', ''),
                    'pattern': pattern,
                    'primary_key': primary_key,  # Th√™m primary key v√†o global group
                    'normalized_command': normalized_cmd,
                    'risk_score': event.get('_risk_score', {})
                }
                print(f"    - üÜï Created new global group")
        
        print(f"  ‚Ä¢ Total global groups after merge: {len(self.global_groups)}")
    
    def should_merge_groups(self, event1: Dict, event2: Dict) -> bool:
        """
        Ki·ªÉm tra nhanh xem 2 events c√≥ n√™n ·ªü c√πng global group kh√¥ng
        
        Args:
            event1: Event th·ª© nh·∫•t
            event2: Event th·ª© hai
        
        Returns:
            True n·∫øu n√™n merge, False n·∫øu kh√¥ng
        """
        similarity_fields = self.config.get("similarity_fields", [])
        similarity_threshold = self.config.get("similarity_threshold", 0.75)
        use_semantic = self.config.get("use_semantic_similarity", False)
        
        # Ch·ªçn function similarity ph√π h·ª£p
        if use_semantic:
            similarity_func = calculate_semantic_similarity
        else:
            similarity_func = calculate_enhanced_similarity if self.config.get("use_enhanced_similarity") else calculate_standard_similarity
        
        # So s√°nh similarity cho t·∫•t c·∫£ similarity fields
        for field in similarity_fields:
            val1 = safe_str(get_nested_value(event1, field))
            val2 = safe_str(get_nested_value(event2, field))
            
            if val1 and val2:
                sim = similarity_func(val1, val2)
                if sim >= similarity_threshold:
                    return True
        
        return False
    
    def cleanup_memory(self) -> None:
        """
        Cleanup memory ƒë·ªÉ tr√°nh memory overflow
        
        Strategy: Gi·ªØ l·∫°i c√°c groups c√≥ nhi·ªÅu events nh·∫•t t·ª´ m·ªói pattern
        """
        if len(self.global_groups) <= MAX_GROUPS_IN_MEMORY:
            return
        
        print(f"üßπ Memory cleanup - Groups: {len(self.global_groups)}")
        
        # Group by pattern first ƒë·ªÉ preserve diversity
        pattern_groups = defaultdict(list)
        for key, data in self.global_groups.items():
            pattern = data.get('pattern', 'unknown')
            pattern_groups[pattern].append((key, data))
        
        # Keep top groups from each pattern
        new_groups = {}
        max_per_pattern = max(1, MAX_GROUPS_IN_MEMORY // len(pattern_groups))
        
        for pattern, groups in pattern_groups.items():
            # Sort by total_events v√† keep top ones
            sorted_groups = sorted(groups, key=lambda x: x[1]['total_events'], reverse=True)
            for key, data in sorted_groups[:max_per_pattern]:
                new_groups[key] = data
        
        removed = len(self.global_groups) - len(new_groups)
        self.global_groups = new_groups
        
        print(f"   Removed {removed} groups, kept {len(new_groups)} across {len(pattern_groups)} patterns")
        gc.collect()  # Force garbage collection
    
    def get_final_results(self) -> List[Dict]:
        """
        Convert global groups th√†nh final results v·ªõi metadata ƒë·∫ßy ƒë·ªß
        
        Returns:
            List c√°c events representative v·ªõi group info v√† risk score
        """
        # print(f"\nüìä DEBUG - Converting {len(self.global_groups)} global groups to final results")
        
        results = []
        for global_key, group_data in self.global_groups.items():
            representative = group_data['representative'].copy()
            
            # Update group info v·ªõi th√¥ng tin t·ª´ global group
            representative['_group_info'].update({
                'total_events_in_group': group_data['total_events'],
                'first_seen': group_data['first_seen'],
                'last_seen': group_data['last_seen'],
                'pattern': group_data['pattern'],
                'primary_key': group_data.get('primary_key', 'unknown'),
                'normalized_command': group_data['normalized_command'],
                'risk_score': group_data['risk_score']
            })
            
            print(f"  ‚Ä¢ {global_key}: {group_data['total_events']} events")
            results.append(representative)
        
        print(f"  ‚Ä¢ Total final results: {len(results)}")
        return results


# --- ENHANCED MAIN DEDUPLICATION FUNCTION ---

def advanced_similarity_dedup_v2(events: List[Dict], config: Dict) -> List[Dict]:
    """
    Enhanced deduplication v·ªõi PRIORITY cho primary fields, commandLine ch·ªâ ƒë·ªÉ refine
    
    Strategy:
    1. STEP 1: Group by PRIMARY FIELDS (parentImage, user, etc.) tr∆∞·ªõc
    2. STEP 2: Trong m·ªói primary group, apply commandLine similarity refinement
    
    M·ª•c ƒë√≠ch: 
    - Tr√°nh over-grouping (t·∫°o qu√° nhi·ªÅu groups nh·ªè)
    - ∆Øu ti√™n grouping theo process v√† user thay v√¨ command line
    - Ch·ªâ s·ª≠ d·ª•ng command line ƒë·ªÉ refine trong primary groups
    
    Args:
        events: List c√°c events c·∫ßn dedup
        config: Config dedup t·ª´ DEDUP_CONFIG_BY_GROUP_ENHANCED
    
    Returns:
        List c√°c events representative (ƒë√£ dedup)
    
    V√≠ d·ª•:
        Input: 300k events c·ªßa forfiles.exe + NT AUTHORITY\SYSTEM
        Step 1: T·∫•t c·∫£ ‚Üí 1 primary group
        Step 2: Trong primary group ‚Üí 2-3 subgroups theo command pattern
        Output: 3-4 groups thay v√¨ 300k groups
    """
    if not events:
        return []
    
    # L·∫•y config parameters
    primary_fields = config.get("primary_fields", [])           # parentImage, user
    similarity_fields = config.get("similarity_fields", [])     # commandLine, parentCommandLine
    similarity_threshold = config.get("similarity_threshold", 0.75)  # Ng∆∞·ª°ng similarity (75%)
    max_samples = config.get("max_samples_per_group", 10)      # S·ªë events t·ªëi ƒëa trong 1 group
    use_semantic = config.get("use_semantic_similarity", False)     # C√≥ d√πng semantic similarity kh√¥ng
    time_window = config.get("time_window_minutes", 120)       # Time window ƒë·ªÉ nh√≥m (2 gi·ªù)
    
    # Ch·ªçn function similarity ph√π h·ª£p
    if use_semantic:
        similarity_func = calculate_semantic_similarity
    else:
        similarity_func = calculate_enhanced_similarity if config.get("use_enhanced_similarity") else calculate_standard_similarity
    
    # Group by time windows first ƒë·ªÉ x·ª≠ l√Ω hi·ªáu qu·∫£ h∆°n
    if time_window > 0:
        time_groups = group_events_by_time(events, time_window)
    else:
        time_groups = {'all': events}
    
    all_deduplicated = []
    
    # X·ª≠ l√Ω t·ª´ng time window
    for time_key, time_events in time_groups.items():
        if not time_events:
            continue
        
        # print(f"\nüìä DEBUG - Processing time window: {time_key} ({len(time_events)} events)")
        
        # STEP 1: Group by PRIMARY FIELDS first (parentImage, user, etc.)
        # M·ª•c ƒë√≠ch: T·∫•t c·∫£ events c√≥ c√πng parentImage + user s·∫Ω ·ªü c√πng 1 group
        primary_groups = defaultdict(list)
        
        for event in time_events:
            # T·∫°o primary key (CH·ªà primary fields, KH√îNG c√≥ commandLine)
            primary_key_parts = []
            for field in primary_fields:
                value = safe_str(get_nested_value(event, field))
                if not value:
                    value = "MISSING"
                primary_key_parts.append(value)
            
            # V√≠ d·ª•: "C:\\Windows\\System32\\forfiles.exe|NT AUTHORITY\\SYSTEM"
            primary_key = "|".join(primary_key_parts)
            primary_groups[primary_key].append(event)
        
        print(f"  ‚Ä¢ Primary groups created: {len(primary_groups)}")
        for key, events_list in primary_groups.items():
            print(f"    - {key}: {len(events_list)} events")
        
        # STEP 2: For each primary group, apply commandLine similarity refinement
        # M·ª•c ƒë√≠ch: Trong c√πng 1 primary group, nh√≥m theo SIMILARITY th·ª±c t·∫ø (kh√¥ng ph·∫£i pattern)
        for primary_key, primary_events in primary_groups.items():
            if len(primary_events) == 1:
                # Single event trong primary group - kh√¥ng c·∫ßn commandLine refinement
                event = primary_events[0].copy()
                event['_group_info'] = {
                    'total_events_in_group': 1,
                    'time_window': time_key,
                    'primary_key': primary_key,                    # parentImage|user
                    'pattern': extract_command_pattern_enhanced(   # command pattern
                        safe_str(get_nested_value(event, 'data.win.eventdata.commandLine'))
                    ),
                    'group_id': f"primary_{hashlib.md5(primary_key.encode()).hexdigest()[:8]}",
                    'first_seen': event.get('@timestamp', ''),
                    'last_seen': event.get('@timestamp', ''),
                    'unique_agents': 1,
                    'dedup_strategy': 'primary_fields_first',     # Strategy: ch·ªâ d√πng primary fields
                    'normalized_command': normalize_command_line_enhanced(
                        safe_str(get_nested_value(event, 'data.win.eventdata.commandLine'))
                    )
                }
                event['_risk_score'] = calculate_risk_score(event, primary_events, config)
                all_deduplicated.append(event)
                continue
            
            # Multiple events trong primary group - apply SIMILARITY-FIRST approach
            # M·ª•c ƒë√≠ch: Nh√≥m c√°c events c√≥ command th·ª±c s·ª± t∆∞∆°ng t·ª± (d·ª±a tr√™n similarity, kh√¥ng ph·∫£i pattern)
            refined_groups = defaultdict(list)
            
            for event in primary_events:
                cmd_line = safe_str(get_nested_value(event, 'data.win.eventdata.commandLine'))
                parent_cmd = safe_str(get_nested_value(event, 'data.win.eventdata.parentCommandLine'))
                
                # T√¨m group t∆∞∆°ng t·ª± nh·∫•t trong c√πng primary group (SIMILARITY-FIRST APPROACH)
                assigned = False
                best_similarity = 0
                best_group = None
                
                for existing_pattern, existing_events in refined_groups.items():
                    representative = existing_events[0]
                    rep_cmd = safe_str(get_nested_value(representative, 'data.win.eventdata.commandLine'))
                    rep_parent_cmd = safe_str(get_nested_value(representative, 'data.win.eventdata.parentCommandLine'))
                    
                    # T√≠nh similarity cho C·∫¢ commandLine V√Ä parentCommandLine
                    cmd_sim = similarity_func(cmd_line, rep_cmd)
                    
                    # T√≠nh similarity cho parentCommandLine n·∫øu c√≥
                    parent_sim = 0
                    if parent_cmd and rep_parent_cmd:
                        parent_sim = similarity_func(parent_cmd, rep_parent_cmd)
                    
                    # L·∫•y similarity cao nh·∫•t gi·ªØa commandLine v√† parentCommandLine
                    current_sim = max(cmd_sim, parent_sim)
                    
                    # C·∫≠p nh·∫≠t best similarity n·∫øu cao h∆°n v√† ƒë·∫°t threshold
                    if current_sim > best_similarity and current_sim >= similarity_threshold:
                        best_similarity = current_sim
                        best_group = existing_pattern
                
                # Assign v√†o group t·ªët nh·∫•t n·∫øu t√¨m th·∫•y
                if best_group:
                    refined_groups[best_group].append(event)
                    assigned = True
                
                # T·∫°o refined group m·ªõi n·∫øu kh√¥ng assign ƒë∆∞·ª£c
                if not assigned:
                    # S·ª≠ d·ª•ng pattern ƒë·ªÉ t·∫°o group m·ªõi (ch·ªâ ƒë·ªÉ organize v√† labeling, KH√îNG ph·∫£i ƒë·ªÉ quy·∫øt ƒë·ªãnh grouping)
                    # Pattern ch·ªâ l√† t√™n g·ªçi cho group, kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn vi·ªác nh√≥m events
                    pattern = extract_command_pattern_enhanced(cmd_line)
                    if pattern not in refined_groups:
                        refined_groups[pattern] = []
                    refined_groups[pattern].append(event)
            
            # T·∫°o final representatives cho m·ªói refined group
            for pattern, pattern_events in refined_groups.items():
                representative = pattern_events[0].copy()
                
                representative['_group_info'] = {
                    'total_events_in_group': len(pattern_events),
                    'time_window': time_key,
                    'primary_key': primary_key,                    # parentImage|user
                    'pattern': pattern,                            # command pattern
                    'group_id': f"primary_{hashlib.md5(primary_key.encode()).hexdigest()[:8]}_{pattern}",
                    'first_seen': min(e.get('@timestamp', '') for e in pattern_events),
                    'last_seen': max(e.get('@timestamp', '') for e in pattern_events),
                    'unique_agents': len(set(e.get('agent', {}).get('name', 'unknown') for e in pattern_events)),
                    'dedup_strategy': 'similarity_first_with_pattern_labeling',  # Strategy: similarity-first, pattern ch·ªâ ƒë·ªÉ label
                    'normalized_command': normalize_command_line_enhanced(
                        safe_str(get_nested_value(representative, 'data.win.eventdata.commandLine'))
                    )
                }
                
                representative['_risk_score'] = calculate_risk_score(representative, pattern_events, config)
                all_deduplicated.append(representative)
    
    return all_deduplicated

# Keep original function as backup
def advanced_similarity_dedup(events: List[Dict], config: Dict) -> List[Dict]:
    """Original deduplication - wrapper to enhanced version"""
    return advanced_similarity_dedup_v2(events, config)


# --- RISK SCORING ---

def calculate_risk_score(representative: Dict, all_events: List[Dict], config: Dict) -> Dict:
    """
    T√≠nh to√°n risk score cho group d·ª±a tr√™n nhi·ªÅu ti√™u ch√≠
    
    M·ª•c ƒë√≠ch: ƒê√°nh gi√° m·ª©c ƒë·ªô r·ªßi ro c·ªßa m·ªôt group events ƒë·ªÉ ∆∞u ti√™n x·ª≠ l√Ω
    
    Args:
        representative: Event ƒë·∫°i di·ªán cho group
        all_events: T·∫•t c·∫£ events trong group
        config: Config dedup
    
    Returns:
        Dictionary ch·ª©a c√°c risk scores:
        - total_score: T·ªïng ƒëi·ªÉm r·ªßi ro (0-1)
        - frequency_score: ƒêi·ªÉm t·∫ßn su·∫•t (0-1)
        - time_score: ƒêi·ªÉm th·ªùi gian (0-1)
        - pattern_risk: ƒêi·ªÉm pattern r·ªßi ro (0-1)
        - agent_diversity: ƒêi·ªÉm ƒëa d·∫°ng agent (0-1)
    
    Scoring logic:
    1. Frequency: C√†ng nhi·ªÅu events c√†ng r·ªßi ro
    2. Time span: C√†ng k√©o d√†i c√†ng r·ªßi ro
    3. Pattern: Commands ƒë√°ng ng·ªù (powershell -enc, rundll32, etc.)
    4. Agent diversity: C√†ng nhi·ªÅu agents kh√°c nhau c√†ng r·ªßi ro
    """
    
    # Frequency score (0-1) - T·∫ßn su·∫•t xu·∫•t hi·ªán
    event_count = len(all_events)
    frequency_score = min(event_count / 1000, 1.0)  # Cap at 1000 events
    
    # Time span score - Kho·∫£ng th·ªùi gian events xu·∫•t hi·ªán
    timestamps = [e.get('@timestamp', '') for e in all_events if e.get('@timestamp')]
    if len(timestamps) > 1:
        try:
            times = [datetime.fromisoformat(t.replace('Z', '+00:00')) for t in timestamps]
            time_span = (max(times) - min(times)).total_seconds() / 3600  # hours
            time_score = min(time_span / 24, 1.0)  # Cap at 24 hours
        except:
            time_score = 0.0
    else:
        time_score = 0.0
    
    # Pattern-based risk - R·ªßi ro d·ª±a tr√™n lo·∫°i command
    pattern_risk = 0.0
    cmd_line = safe_str(get_nested_value(representative, 'data.win.eventdata.commandLine'))
    
    # Danh s√°ch c√°c indicators ƒë√°ng ng·ªù
    suspicious_indicators = [
        'powershell -enc', 'invoke-webrequest', 'downloadstring',
        'rundll32', 'regsvr32', 'mshta', 'wscript', 'cscript',
        'certutil -decode', 'bitsadmin', 'net user', 'whoami'
    ]
    
    # C·ªông ƒëi·ªÉm r·ªßi ro cho m·ªói indicator
    for indicator in suspicious_indicators:
        if indicator in cmd_line.lower():
            pattern_risk += 0.2
    
    pattern_risk = min(pattern_risk, 1.0)  # Cap at 1.0
    
    # Agent diversity (nhi·ªÅu agents = riskier) - ƒêa d·∫°ng agent
    unique_agents = len(set(e.get('agent', {}).get('name', 'unknown') for e in all_events))
    agent_score = min(unique_agents / 10, 1.0)  # Cap at 10 agents
    
    # Combined score - K·∫øt h·ª£p c√°c ƒëi·ªÉm v·ªõi tr·ªçng s·ªë
    final_score = (frequency_score * 0.3 + time_score * 0.2 + pattern_risk * 0.4 + agent_score * 0.1)
    
    return {
        'total_score': round(final_score, 3),
        'frequency_score': round(frequency_score, 3), 
        'time_score': round(time_score, 3),
        'pattern_risk': round(pattern_risk, 3),
        'agent_diversity': round(agent_score, 3)
    }


# --- DATA FETCHING ---

def fetch_events_in_chunks(rule_id: str, password: str) -> Generator[List[Dict], None, None]:
    """
    Fetch events t·ª´ Wazuh Indexer theo chunks ƒë·ªÉ x·ª≠ l√Ω hi·ªáu qu·∫£
    
    M·ª•c ƒë√≠ch: 
    1. Tr√°nh memory overflow khi x·ª≠ l√Ω h√†ng tri·ªáu events
    2. X·ª≠ l√Ω streaming - x·ª≠ l√Ω t·ª´ng chunk m·ªôt
    3. S·ª≠ d·ª•ng pagination v·ªõi search_after ƒë·ªÉ l·∫•y d·ªØ li·ªáu li√™n t·ª•c
    
    Args:
        rule_id: ID c·ªßa rule c·∫ßn l·∫•y events
        password: Password cho Wazuh Indexer
    
    Yields:
        List c√°c events (m·ªói chunk c√≥ th·ªÉ c√≥ 5000 events)
    
    Strategy:
    1. S·ª≠ d·ª•ng search_after token ƒë·ªÉ pagination
    2. M·ªói request l·∫•y FETCH_SIZE_PER_PAGE events
    3. Ti·∫øp t·ª•c cho ƒë·∫øn khi kh√¥ng c√≤n events
    """
    search_url = f"{INDEXER_DOMAIN}/wazuh-alert*/_search"
    headers = {'Content-Type': 'application/json', 'User-Agent': USER_AGENT}
    search_after_token = None
    
    print(f"\n--- [ Rule ID: {rule_id} ] ---")
    print(f"üöÄ B·∫Øt ƒë·∫ßu enhanced streaming processing...")

    while True:
        # T·∫°o query body cho Elasticsearch
        query_body = {
            "size": FETCH_SIZE_PER_PAGE,  # S·ªë events m·ªói request
            "query": {
                "bool": {
                    "must": [
                        {"match": {"rule.id": rule_id}},           # Filter theo rule ID
                        {"range": {"@timestamp": {"gte": TIME_RANGE}}}  # Filter theo th·ªùi gian (UTC+7, Gi·ªù Vi·ªát Nam)
                    ]
                }
            },
            "sort": [{"@timestamp": "desc"}, {"_id": "asc"}]  # Sort ƒë·ªÉ pagination
        }

        # Th√™m search_after token n·∫øu c√≥ (pagination)
        if search_after_token:
            query_body["search_after"] = search_after_token
        
        try:
            # G·ª≠i request ƒë·∫øn Wazuh Indexer
            response = requests.get(
                search_url, 
                auth=(INDEXER_USER, password), 
                headers=headers, 
                json=query_body, 
                verify=False,  # T·∫Øt SSL verification
                timeout=180    # Timeout 3 ph√∫t
            )
            response.raise_for_status()
            data = response.json()
            hits = data.get('hits', {}).get('hits', [])

            if not hits:
                print("‚ÑπÔ∏è Kh√¥ng c√≤n events ƒë·ªÉ t·∫£i.")
                break

            # Yield events t·ª´ chunk hi·ªán t·∫°i
            yield [hit['_source'] for hit in hits]
            
            # L·∫•y search_after token cho chunk ti·∫øp theo
            search_after_token = hits[-1]['sort']
            
        except Exception as e:
            print(f"‚ùå L·ªói khi t·∫£i d·ªØ li·ªáu: {e}")
            break


# --- ENHANCED ANALYSIS & REPORTING ---

def analyze_rule_streaming_v2(rule_id: str, password: str):
    """
    Enhanced streaming analysis - H√†m ch√≠nh x·ª≠ l√Ω analysis cho 1 rule
    
    M·ª•c ƒë√≠ch: 
    1. Fetch events t·ª´ Wazuh Indexer theo chunks
    2. √Åp d·ª•ng deduplication v·ªõi primary fields first strategy
    3. Merge results t·ª´ c√°c chunks
    4. T·∫°o b√°o c√°o chi ti·∫øt
    
    Args:
        rule_id: ID c·ªßa rule c·∫ßn analyze
        password: Password cho Wazuh Indexer
    
    Execution Flow:
    1. Fetch events theo chunks (5000 events/chunk)
    2. X√°c ƒë·ªãnh config dedup cho lo·∫°i event
    3. Kh·ªüi t·∫°o StreamingDeduplicatorV2
    4. X·ª≠ l√Ω t·ª´ng chunk v·ªõi advanced_similarity_dedup_v2
    5. Merge results v√†o global groups
    6. Cleanup memory ƒë·ªãnh k·ª≥
    7. T·∫°o final results v√† b√°o c√°o
    """
    print(f"üìä ƒêang ph√¢n t√≠ch Rule ID: {rule_id} (Enhanced Streaming Mode)")
    
    deduplicator = None
    total_events_processed = 0
    config = None
    
    # X·ª≠ l√Ω t·ª´ng chunk events
    for chunk_number, chunk in enumerate(fetch_events_in_chunks(rule_id, password), 1):
        total_events_processed += len(chunk)
        print(f"  -> Chunk {chunk_number}: {len(chunk)} events | Total: {total_events_processed:,}")
        
        # X√°c ƒë·ªãnh config dedup cho chunk ƒë·∫ßu ti√™n
        if config is None and chunk:
            # S·ª≠ d·ª•ng enhanced config d·ª±a tr√™n rule groups
            base_config = determine_dedup_config(chunk[0])
            config = DEDUP_CONFIG_BY_GROUP_ENHANCED.get(
                base_config['event_type'], 
                DEDUP_CONFIG_BY_GROUP_ENHANCED["default"]
            )
            deduplicator = StreamingDeduplicatorV2(config)
            print(f"  -> Enhanced dedup strategy: {config['event_type']}")
            print(f"  -> Similarity threshold: {config['similarity_threshold']}")
            print(f"  -> Semantic similarity: {config.get('use_semantic_similarity', False)}")
            print(f"  -> Primary fields: {config.get('primary_fields', [])}")
            print(f"  -> Similarity fields: {config.get('similarity_fields', [])}")
        
        if deduplicator:
            # √Åp d·ª•ng enhanced dedup function cho chunk hi·ªán t·∫°i
            chunk_groups = advanced_similarity_dedup_v2(chunk, config)
            
            # Merge results v√†o global groups
            deduplicator.merge_groups_into_global(chunk_groups)
            deduplicator.processed_count = total_events_processed
            
            # Cleanup memory ƒë·ªãnh k·ª≥
            if total_events_processed % MEMORY_CLEANUP_INTERVAL == 0:
                deduplicator.cleanup_memory()
            
            # Gi·∫£i ph√≥ng memory cho chunk hi·ªán t·∫°i
            del chunk, chunk_groups
            gc.collect()
        
        # Ki·ªÉm tra gi·ªõi h·∫°n s·ªë events x·ª≠ l√Ω
        if total_events_processed >= MAX_EVENTS_TO_PROCESS:
            print(f"  -> Reached limit {MAX_EVENTS_TO_PROCESS:,} events.")
            break
    
    # Ki·ªÉm tra k·∫øt qu·∫£
    if not deduplicator or not deduplicator.global_groups:
        print("‚úÖ No events or groups found.")
        return
    
    # L·∫•y final results
    final_results = deduplicator.get_final_results()
    
    # Enhanced reporting
    print(f"\n--- ENHANCED DEDUPLICATION REPORT ---")
    print(f"Rule ID: {rule_id}")
    print(f"Total events processed: {total_events_processed:,}")
    print(f"Unique behaviors found: {len(final_results):,}")
    print(f"Reduction ratio: {(1 - len(final_results)/max(total_events_processed, 1))*100:.1f}%")
    
    # Pattern breakdown - Ph√¢n t√≠ch theo patterns
    pattern_counts = defaultdict(int)
    total_events_by_pattern = defaultdict(int)
    for result in final_results:
        pattern = result.get('_group_info', {}).get('pattern', 'unknown')
        pattern_counts[pattern] += 1
        total_events_by_pattern[pattern] += result.get('_group_info', {}).get('total_events_in_group', 1)
    
    print(f"\n--- PATTERN BREAKDOWN ---")
    for pattern in sorted(pattern_counts.keys()):
        groups = pattern_counts[pattern]
        events = total_events_by_pattern[pattern]
        print(f"  {pattern}: {groups:,} groups ({events:,} total events)")
    
    # T·∫°o b√°o c√°o chi ti·∫øt
    create_comprehensive_report_v2(final_results, rule_id, config)

def create_comprehensive_report_v2(events: List[Dict], rule_id: str, config: Dict):
    """Enhanced reporting v·ªõi better insights v√† primary fields focus"""
    if not events:
        return
    
    events_sorted = sorted(events, key=lambda x: (
        x.get('_risk_score', {}).get('total_score', 0),
        x.get('_group_info', {}).get('total_events_in_group', 0)
    ), reverse=True)
    
    summary_data = []
    
    for event in events_sorted:
        group_info = event.get('_group_info', {})
        risk_score = event.get('_risk_score', {})
        
        row = {
            "event_count": group_info.get('total_events_in_group', 1),
            "risk_score": risk_score.get('total_score', 0),
            "pattern": group_info.get('pattern', 'unknown'),
            "primary_key": group_info.get('primary_key', 'N/A')[:80] + ('...' if len(group_info.get('primary_key', '')) > 80 else ''),
            "first_seen": group_info.get('first_seen', 'N/A')[:19],
            "last_seen": group_info.get('last_seen', 'N/A')[:19],
            "dedup_strategy": group_info.get('dedup_strategy', 'unknown'),
            "normalized_command": group_info.get('normalized_command', 'N/A')[:100] + ('...' if len(group_info.get('normalized_command', '')) > 100 else '')
        }
        
        # Add key fields based on config
        for field in config.get("primary_fields", []):
            value = safe_str(get_nested_value(event, field))
            field_name = field.split('.')[-1]
            row[field_name] = value[:50] + ('...' if len(value) > 50 else '')
        
        # Add original command line for reference
        original_cmd = safe_str(get_nested_value(event, 'data.win.eventdata.commandLine'))
        row["original_command_sample"] = original_cmd[:80] + ('...' if len(original_cmd) > 80 else '')
        
        summary_data.append(row)
    
    df = pd.DataFrame(summary_data)
    
    print(f"\n--- TOP BEHAVIORS (Primary Fields First Grouping) ---")
    print(df.head(15).to_string(index=False))
    
    # Export
    try:
        filename = f"enhanced_dedup_rule_{rule_id}_{datetime.now().strftime('%Y%m%d_%H%M')}.csv"
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"\nüíæ Enhanced report exported: {filename}")
    except Exception as e:
        print(f"\n‚ùå Export error: {e}")
    
    # Primary fields grouping analysis
    print(f"\n--- PRIMARY FIELDS GROUPING ANALYSIS ---")
    primary_key_counts = df.groupby('primary_key')['event_count'].sum().sort_values(ascending=False)
    print(f"Top primary field combinations:")
    for primary_key, total_events in primary_key_counts.head(10).items():
        print(f"  ‚Ä¢ {total_events:,} events: {primary_key}")
    
    # Pattern breakdown within primary groups
    print(f"\n--- PATTERN BREAKDOWN WITHIN PRIMARY GROUPS ---")
    pattern_stats = df.groupby(['primary_key', 'pattern']).agg({
        'event_count': ['sum', 'count']
    }).round(3)
    
    pattern_stats.columns = ['total_events', 'groups']
    print(pattern_stats.head(20))
    
    # Detailed statistics by pattern
    print(f"\n--- DETAILED PATTERN STATISTICS ---")
    pattern_stats_overall = df.groupby('pattern').agg({
        'event_count': ['count', 'sum', 'mean', 'max'],
        'risk_score': ['mean', 'max']
    }).round(3)
    
    pattern_stats_overall.columns = ['groups', 'total_events', 'avg_events_per_group', 'max_events_in_group', 'avg_risk', 'max_risk']
    print(pattern_stats_overall)
    
    # High volume groups analysis
    high_volume_groups = df[df['event_count'] >= 1000]
    if not high_volume_groups.empty:
        print(f"\n--- HIGH VOLUME GROUPS (>=1000 events) ---")
        print(f"Found {len(high_volume_groups)} high-volume groups:")
        for _, row in high_volume_groups.head(10).iterrows():
            print(f"  ‚Ä¢ {row['event_count']:,} events - {row['pattern']} - {row['primary_key']}")
    
    # Dedup strategy effectiveness
    print(f"\n--- DEDUPLICATION STRATEGY EFFECTIVENESS ---")
    strategy_stats = df.groupby('dedup_strategy').agg({
        'event_count': ['sum', 'count']
    })
    strategy_stats.columns = ['total_events', 'groups']
    print(strategy_stats)


# --- LEGACY FUNCTIONS (Keep for compatibility) ---

class StreamingDeduplicator:
    """Legacy class - redirects to enhanced version"""
    def __init__(self, config: Dict):
        self.enhanced = StreamingDeduplicatorV2(config)
        self.global_groups = self.enhanced.global_groups
        self.processed_count = self.enhanced.processed_count
    
    def merge_groups_into_global(self, chunk_groups: List[Dict]) -> None:
        return self.enhanced.merge_groups_into_global(chunk_groups)
    
    def cleanup_memory(self) -> None:
        return self.enhanced.cleanup_memory()
    
    def get_final_results(self) -> List[Dict]:
        return self.enhanced.get_final_results()

def analyze_rule_streaming(rule_id: str, password: str):
    """Legacy function - redirects to enhanced version"""
    return analyze_rule_streaming_v2(rule_id, password)

def create_comprehensive_report(events: List[Dict], rule_id: str, config: Dict):
    """Legacy function - redirects to enhanced version"""
    return create_comprehensive_report_v2(events, rule_id, config)


# --- MAIN FUNCTIONS ---

def main_enhanced():
    """Enhanced main function v·ªõi Primary Fields First strategy"""
    print("=" * 70)
    print("     ENHANCED STREAMING LOG DEDUPLICATION TOOL")
    print("     PRIMARY FIELDS FIRST STRATEGY")
    print("=" * 70)
    print(f"üíæ Memory limit: {MAX_GROUPS_IN_MEMORY:,} groups")
    print(f"‚ö° Chunk size: {CHUNK_SIZE:,} events")
    print(f"üéØ Max processing: {MAX_EVENTS_TO_PROCESS:,} events")
    print(f"üß† Enhanced deduplication: ENABLED")
    print(f"üîç Semantic similarity: ENABLED")
    print(f"üéØ PRIMARY FIELDS FIRST: ENABLED (parentImage + user)")
    print(f"üìä CommandLine similarity: SECONDARY (refinement only)")
    print(f"‚öñÔ∏è Similarity threshold: 0.75 (reduced for better grouping)")
    print(f"üïê Timezone: UTC+7 (Gi·ªù Vi·ªát Nam)")
    print(f"üìÖ Time range: {TIME_RANGE}")
    
    try:
        password = getpass(f"Nh·∫≠p m·∫≠t kh·∫©u cho user '{INDEXER_USER}': ")
    except Exception as e:
        print(f"‚ùå Password input error: {e}")
        return
    
    for rule_id in RULES_TO_MONITOR:
        try:
            analyze_rule_streaming_v2(rule_id, password)
            print("-" * 70)
        except Exception as e:
            print(f"‚ùå Error analyzing rule {rule_id}: {e}")
            import traceback
            traceback.print_exc()
    
    print("\n‚úÖ Enhanced streaming analysis completed!")
    print("üìä Results now prioritize primary fields (parentImage, user) over commandLine patterns")
    print(f"üïê Data queried using UTC+7 timezone (Gi·ªù Vi·ªát Nam)")
    print(f"üìÖ Time range: {TIME_RANGE}")

def main():
    """Original main function - now uses enhanced logic"""
    return main_enhanced()


if __name__ == "__main__":
    main()